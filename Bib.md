References

### 1. Boob D, Cummings R, Kimpara D, Tantipongpipat UT, Waites C, Zimmerman K. Differentially Private Synthetic Data Generation via GANs.  Extended abstract, TPDP 2018.
Use the same approach as (4). Provide some optimization techniques like smart clipping of weights, using some public data to start the GAN training as training is unstable in the beginning etc to improve GAN training and accuracy.
### 2. XinyangZhang,ShoulingJi,andTingWang.2018.Differentially Private Releasing via Deep Generative Model.arXiv preprint 1801.01594.
Did not find any difference between this and (4).
### 3. Jinsung Yoon, James Jordon, Mihaela van der Schaar. PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees. ICLR 2019.
Use the PATE framework for training GANs. Use Student and teacher discriminators. The student is trained to imitate the teacher but is only provided the samples generated by the Generator. The generator in turn receives feedback from the student discriminator. The privacy budget is utilized in labelling the generated samples by the teacher using PATE. Propose interesting metrics to evaluate the resulting generative model by comparing performances of classifers on real data vs synthetic data. Improves on (4).
### 4. Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, Jiayu Zhou. Differentially Private Generative Adversarial Network. arXiv preprint 1802.06739
Use the DP-SGD proposed in the Deep learning with Differential Privacy (Abadi et al) to train the discriminator of a GAN. The generator also becomes differentially private due to the post processing theorem of differential privacy.
### 5. Matej Balog, Ilya Tolstikhin, Bernhard Schï¿½lkopf. Differentially Private Database Release via Kernel Mean Embeddings. ICML 2018
They compute the emprircal kernel mean embedding of the dataset, inject noise in it and propose two algorithms to generate synthetic data points using the noisy kernel mean embedding. The paper is quite theoretical but their experimental section is rather weak, lacking evaluation of their method on real datasets.
### 6. Gergely Acs, Luca Melis, Claude Castelluccia, Emiliano De Cristofaro. Differentially Private Mixture of Generative Neural Networks. TKDE 2017
Use a differentially private version of kernel K-means to cluster the data and then train 'k' different generative models on these clusters. Hope is that since the data is clustered, the models converge faster so require less epochs hence less privacy budget. The sensitivity is also reduced as removal of a record impacts only a few clusters.
